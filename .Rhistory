if (!file.exists("data-raw/reviews")) {
tmp <- tempfile(fileext = ".tar.gz")
download.file("http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz",
tmp, quiet = TRUE)
untar(tmp, exdir = "data-raw/reviews")
unlink(tmp)
}
path <- file.path("data-raw", "reviews", "txt_sentoken")
pos <- list.files(file.path(path, "pos"))
neg <- list.files(file.path(path, "neg"))
pos.files <- file.path(path, "pos", pos)
neg.files <- file.path(path, "neg", neg)
all.files <- c(pos.files, neg.files)
txt <- lapply(all.files, readLines)
nms <- gsub("data-raw/reviews/txt_sentoken", "", all.files)
reviews <- setNames(txt, nms)
reviews <- sapply(reviews, function(x) paste(x, collapse = " "))
save(reviews, file = "data/reviews.rdata", compress = "xz")
path <- file.path("data-raw", "reviews", "txt_sentoken")
pos <- list.files(file.path(path, "pos"))
neg <- list.files(file.path(path, "neg"))
pos.files <- file.path(path, "pos", pos)
neg.files <- file.path(path, "neg", neg)
all.files <- c(pos.files, neg.files)
txt <- lapply(all.files, readLines)
nms <- gsub("data-raw/reviews/txt_sentoken", "", all.files)
reviews <- setNames(txt, nms)
reviews <- sapply(reviews, function(x) paste(x, collapse = " "))
q()
load("C:/Users/Nihit/OneDrive/PhD/Projects/ADB_Evaluation/R code/data/evaluations.rdata")
#Load the text mining package
library(tm)
#Read PCRs into R
pcr <- read.csv("input\\ADB_Comprehensive_PCR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the lessons column
lessons <- pcr[["LESSONS"]]
corpus <- Corpus(VectorSource(lessons))
writeLines(as.character(corpus[[10]]))
#Preprocess contents of lessons for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
writeLines(as.character(corpus[[10]]))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
additionalStopWords <- c("project", "lesson", "topic", "adb", "para", "text", "year", "document", "amp")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pcr[["Report Title"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PCRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "frequencies.csv")
library(topicmodels)
library(ldatuning)
result <- FindTopicsNumber(
dtm,
topics = seq(from = 6, to = 39, by = 3),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = 1,
verbose = TRUE
)
FindTopicsNumber_plot(result)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <- list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 10
ldaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
ldaOutput.topics <- as.matrix(topics(ldaOutput))
write.csv(ldaOutput.topics, file = paste("LDAGibbs", k, "topics_by_pcr.csv"))
ldaOutput.terms <- as.matrix(terms(ldaOutput, 20))
write.csv(ldaOutput.terms, file = paste("LDAGibbs", k, "terms_by_topics.csv"))
topicProbabilities <- as.data.frame(ldaOutput@gamma)
write.csv(topicProbabilities, file = paste("LDAGibbs", k, "topic_probabilities.csv"))
#Load the text mining package
library(tm)
#Read PCRs into R
pcr <- read.csv("input\\ADB_Comprehensive_PCR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the lessons column
lessons <- pcr[["LESSONS"]]
corpus <- Corpus(VectorSource(lessons))
writeLines(as.character(corpus[[10]]))
#Preprocess contents of lessons for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
writeLines(as.character(corpus[[10]]))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
additionalStopWords <- c("project", "lesson", "topic", "adb", "para", "text", "year", "document", "amp")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pcr[["Report Title"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PCRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "frequencies.csv")
library(topicmodels)
library(ldatuning)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <- list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
result <- FindTopicsNumber(
dtm,
topics = seq(from = 10, to = 100, by = 10),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin),
mc.cores = 1,
verbose = TRUE
)
FindTopicsNumber_plot(result)
#Load the text mining package
library(tm)
#Read PVRs into R
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the lessons column
lessons <- pvr[["LESSONS"]]
corpus <- Corpus(VectorSource(lessons))
writeLines(as.character(corpus[[10]]))
#Preprocess contents of lessons for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
writeLines(as.character(corpus[[10]]))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
additionalStopWords <- c("project", "report", "pcr", "lesson", "topic", "adb", "para", "text", "year", "document", "amp")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pvr[["Report Title"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PVR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "frequencies.csv")
library(topicmodels)
library(ldatuning)
#Set parameters for Gibbs sampling
burnin <- 100
iter <- 1000
thin <- 50
nstart <- 5
seed <- sample(1:1000000, 5)
best <- TRUE
t1 <- Sys.time()
#result <- FindTopicsNumber(
#  dtm,
#  topics = seq(from = 10, to = 20, by = 2),
#  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
#  method = "Gibbs",
#  control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin),
#  mc.cores = 1,
#  verbose = TRUE
#)
#FindTopicsNumber_plot(result)
#t2 <- Sys.time()
#t2 - t1
best.model <- lapply(seq(12,60, by=3), function(k){LDA(dtm, k)})
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))
best.model.logLik.df <- data.frame(topics=seq(12,60, by=3), LL=as.numeric(as.matrix(best.model.logLik)))
t2 <- Sys.time()
t2 - t1
library(ggplot2)
ggplot(best.model.logLik.df, aes(x=topics, y=LL)) +
xlab("Number of topics") + ylab("Log likelihood of the model") +
geom_line() +
theme_bw() #+
#  opts(axis.title.x = theme_text(vjust = -0.25, size = 14)) +
#  opts(axis.title.y = theme_text(size = 14, angle=90))
best.model.logLik.df[which.max(best.model.logLik.df$LL),]
#k <- 100
#ldaOutput <- LDA(dtm, k, method="Gibbs",
#                 control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
#                                iter = iter, thin = thin))
#
# ldaOutput.topics <- as.matrix(topics(ldaOutput))
# write.csv(ldaOutput.topics, file = paste("LDAGibbs", k, "topics_by_pcr.csv"))
#
# ldaOutput.terms <- as.matrix(terms(ldaOutput, 20))
# write.csv(ldaOutput.terms, file = paste("LDAGibbs", k, "terms_by_topics.csv"))
#
# topicProbabilities <- as.data.frame(ldaOutput@gamma)
# write.csv(topicProbabilities, file = paste("LDAGibbs", k, "topic_probabilities.csv"))
pcr <- read.csv("input\\ADB_Comprehensive_PCR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
pcr <- read.csv("input\\ADB_Comprehensive_PCR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
setwd("C:/Users/Nihit/OneDrive/PhD/Projects/ADB_Evaluation/Code/R/ADBEvaluation")
pcr <- read.csv("input\\ADB_Comprehensive_PCR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pcr[["LESSONS"]]
library(tm)
corpus <- Corpus(VectorSource(lessons))
#Preprocess contents of titles for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
additionalStopWords <- c("adb", "amp", "completion", "document", "lesson", "para", "pcr",
"program", "project", "report", "text", "topic", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pcr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
library(tm)
corpus <- Corpus(VectorSource(lessons))
#Preprocess contents of titles for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
additionalStopWords <- c("adb", "amp", "completion", "document", "lesson", "para", "pcr",
"program", "project", "report", "text", "topic", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pcr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies.csv")
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
library(tm)
corpus <- Corpus(VectorSource(lessons))
#Preprocess contents of titles for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
additionalStopWords <- c("adb", "amp", "completion", "document", "lesson", "para", "pcr",
"program", "project", "report", "text", "topic", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pcr[["ID"]]
rownames(dtm) <- pvr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies.csv")
additionalStopWords <- c("adb", "amp", "completion", "document", "lesson", "lessons", "para", "pcr",
"program", "project", "projects", "report", "text", "topic", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pvr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies.csv")
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
library(tm)
corpus <- Corpus(VectorSource(lessons))
#Preprocess contents of titles for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
additionalStopWords <- c("adb", "amp", "completion", "document", "lesson", "lessons", "para", "pcr",
"program", "project", "projects", "report", "text", "topic", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
#corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pvr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies.csv")
library(ldatuning)
#Set parameters for Gibbs sampling
burnin <- 100
iter <- 500
thin <- 50
nstart <- 5
seed <- sample(1:1000000, 5)
best <- TRUE
t1 <- Sys.time()
testLdaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
t2 <- Sys.time()
t2 - t1
library(topicmodels)
t1 <- Sys.time()
testLdaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
t2 <- Sys.time()
t2 - t1
k = 10
t1 <- Sys.time()
testLdaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
t2 <- Sys.time()
t2 - t1
t1 <- Sys.time()
best.model <- lapply(seq(5,30, by=1), function(k){LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best,
burnin = burnin, iter = iter, thin = thin))})
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))
best.model.logLik.df <- data.frame(topics=seq(5,35, by=1), LL=as.numeric(as.matrix(best.model.logLik)))
t2 <- Sys.time()
t2 - t1
t1 <- Sys.time()
best.model <- lapply(seq(5,35, by=1), function(k){LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best,
burnin = burnin, iter = iter, thin = thin))})
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))
best.model.logLik.df <- data.frame(topics=seq(5,35, by=1), LL=as.numeric(as.matrix(best.model.logLik)))
t2 <- Sys.time()
t2 - t1
library(ggplot2)
ggplot(best.model.logLik.df, aes(x=topics, y=LL)) +
xlab("Number of topics") + ylab("Log likelihood of the model") + geom_line() + theme_bw()
best.model.logLik.df[which.max(best.model.logLik.df$LL),]
t1 <- Sys.time()
ldaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
t2 <- Sys.time()
t2 - t1
df<-data.frame(topics=c(1:500), LL=as.numeric(as.matrix(ldaOutput$log.likelihood[1,])))
ggplot(df, aes(x=topics, y=LL)) +
xlab("iteration") + ylab("Log likelihood of the model") +
geom_line() +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.25, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90))
df<-data.frame(topics=c(1:500), LL=as.numeric(as.matrix(ldaOutput@log.likelihood[1,])))
ggplot(df, aes(x=topics, y=LL)) +
xlab("iteration") + ylab("Log likelihood of the model") +
geom_line() +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.25, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90))
