corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pcr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies.csv")
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
library(tm)
corpus <- Corpus(VectorSource(lessons))
#Preprocess contents of titles for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
additionalStopWords <- c("adb", "amp", "completion", "document", "lesson", "para", "pcr",
"program", "project", "report", "text", "topic", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pcr[["ID"]]
rownames(dtm) <- pvr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies.csv")
additionalStopWords <- c("adb", "amp", "completion", "document", "lesson", "lessons", "para", "pcr",
"program", "project", "projects", "report", "text", "topic", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pvr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies.csv")
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
library(tm)
corpus <- Corpus(VectorSource(lessons))
#Preprocess contents of titles for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
additionalStopWords <- c("adb", "amp", "completion", "document", "lesson", "lessons", "para", "pcr",
"program", "project", "projects", "report", "text", "topic", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
#corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "test.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pvr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies.csv")
library(ldatuning)
#Set parameters for Gibbs sampling
burnin <- 100
iter <- 500
thin <- 50
nstart <- 5
seed <- sample(1:1000000, 5)
best <- TRUE
t1 <- Sys.time()
testLdaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
t2 <- Sys.time()
t2 - t1
library(topicmodels)
t1 <- Sys.time()
testLdaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
t2 <- Sys.time()
t2 - t1
k = 10
t1 <- Sys.time()
testLdaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
t2 <- Sys.time()
t2 - t1
t1 <- Sys.time()
best.model <- lapply(seq(5,30, by=1), function(k){LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best,
burnin = burnin, iter = iter, thin = thin))})
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))
best.model.logLik.df <- data.frame(topics=seq(5,35, by=1), LL=as.numeric(as.matrix(best.model.logLik)))
t2 <- Sys.time()
t2 - t1
t1 <- Sys.time()
best.model <- lapply(seq(5,35, by=1), function(k){LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best,
burnin = burnin, iter = iter, thin = thin))})
best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))
best.model.logLik.df <- data.frame(topics=seq(5,35, by=1), LL=as.numeric(as.matrix(best.model.logLik)))
t2 <- Sys.time()
t2 - t1
library(ggplot2)
ggplot(best.model.logLik.df, aes(x=topics, y=LL)) +
xlab("Number of topics") + ylab("Log likelihood of the model") + geom_line() + theme_bw()
best.model.logLik.df[which.max(best.model.logLik.df$LL),]
t1 <- Sys.time()
ldaOutput <- LDA(dtm, k, method="Gibbs",
control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
iter = iter, thin = thin))
t2 <- Sys.time()
t2 - t1
df<-data.frame(topics=c(1:500), LL=as.numeric(as.matrix(ldaOutput$log.likelihood[1,])))
ggplot(df, aes(x=topics, y=LL)) +
xlab("iteration") + ylab("Log likelihood of the model") +
geom_line() +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.25, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90))
df<-data.frame(topics=c(1:500), LL=as.numeric(as.matrix(ldaOutput@log.likelihood[1,])))
ggplot(df, aes(x=topics, y=LL)) +
xlab("iteration") + ylab("Log likelihood of the model") +
geom_line() +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.25, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90))
plot.STM(stmOutput, type="summary")
library(stm)
library(stm)
pcr$year <- as.numeric( format(as.Date(pcr$Report.Approval.Date., "%d-%m-%y"), "%Y"))
stmInput <- textProcessor(pcr$Report.Title, metadata = pcr, customstopwords = additionalStopWords)
stmData <- prepDocuments( stmInput$documents, stmInput$vocab, stmInput$meta, lower.thresh = 0)
pcr <- read.csv("input\\ADB_Comprehensive_PCR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the Project Title column
titles <- pcr[["Report.Title"]]
library(stm)
pcr$year <- as.numeric( format(as.Date(pcr$Report.Approval.Date., "%d-%m-%y"), "%Y"))
stmInput <- textProcessor(pcr$Report.Title, metadata = pcr, customstopwords = additionalStopWords)
stmData <- prepDocuments( stmInput$documents, stmInput$vocab, stmInput$meta, lower.thresh = 0)
k <- 12
#t1 <- Sys.time()
#Use this when not using "Spectral" init
#stmSelect <- selectModel(stmData$documents, stmData$vocab, K = k,
#                         prevalence =~  I..CHOOSE.SECTOR, max.em.its = 200,
#                         data = stmData$meta, runs = 20, seed = sample(1:100000))
#t2 <- Sys.time()
#t2 - t1
#plotModels(stmSelect)
stmOutput <- stm(stmData$documents, stmData$vocab, K = k,
prevalence =~  I..CHOOSE.SECTOR + Choose.COUNTRIES + s(year),
max.em.its = 500, data = stmData$meta, init.type = "Spectral")
additionalStopWords <- c("completion", "development", "program", "project", "report", "sector")
library(stm)
pcr$year <- as.numeric( format(as.Date(pcr$Report.Approval.Date., "%d-%m-%y"), "%Y"))
stmInput <- textProcessor(pcr$Report.Title, metadata = pcr, customstopwords = additionalStopWords)
stmData <- prepDocuments( stmInput$documents, stmInput$vocab, stmInput$meta, lower.thresh = 0)
k <- 12
#t1 <- Sys.time()
#Use this when not using "Spectral" init
#stmSelect <- selectModel(stmData$documents, stmData$vocab, K = k,
#                         prevalence =~  I..CHOOSE.SECTOR, max.em.its = 200,
#                         data = stmData$meta, runs = 20, seed = sample(1:100000))
#t2 <- Sys.time()
#t2 - t1
#plotModels(stmSelect)
stmOutput <- stm(stmData$documents, stmData$vocab, K = k,
prevalence =~  I..CHOOSE.SECTOR + Choose.COUNTRIES + s(year),
max.em.its = 500, data = stmData$meta, init.type = "Spectral")
plot.STM(stmOutput, type="summary")
stmEffect <- estimateEffect(seq(1:k) ~ I..CHOOSE.SECTOR + Choose.COUNTRIES + s(year), stmOutput,
metadata=stmData$meta, uncertainty = "None")
plot.estimateEffect(stmEffect, covariate = "year", model = stmOutput, method = "continuous", ci.level = 0,
topics = seq(1:k))
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv",
header = TRUE,
quote="\"",
stringsAsFactors= TRUE,
strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
library(tm)
corpus <- Corpus(VectorSource(lessons))
#Preprocess contents of titles for topics modelling
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
additionalStopWords <- c("adb", "amp", "completion report", "document", "lesson", "lessons", "para", "pcr",
"program", "project", "project completion report", "projects", "report", "text",
"topic", "topics", "xarr", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "pvr_lessons_stem.txt")
dtm <- DocumentTermMatrix(corpus)
rownames(dtm) <- pvr[["ID"]]
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each PCR
dtm <- dtm[rowTotals > 0, ] #Remove all PVRs without words
frequency <- colSums(as.matrix(dtm))
length(frequency)
order <- order(frequency, decreasing = TRUE)
frequency[order]
write.csv(frequency[order], "output\\frequencies_stem.csv")
library(ldatuning)
#Set parameters for Gibbs sampling
#burnin <- 100
#iter <- 1000
#thin <- 50
#nstart <- 5
#seed <- sample(1:1000000, 5)
#best <- TRUE
t1 <- Sys.time()
result <- FindTopicsNumber(
dtm,
topics = seq(from = 5, to = 205, by = 5),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
#  control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin),
mc.cores = 1,
verbose = TRUE
)
t2 <- Sys.time()
t2 - t1
View(result)
View(result)
FindTopicsNumber_plot(result)
t1 <- Sys.time()
result <- FindTopicsNumber(
dtm,
topics = seq(from = 20, to = 55, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
#  control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin),
mc.cores = 1,
verbose = TRUE
)
t2 <- Sys.time()
t2 - t1
View(result)
FindTopicsNumber_plot(result)
k <- 26
library(topicmodels)
t1 <- Sys.time()
ldaOutput <- LDA(dtm, k, method="Gibbs",
#control = list(nstart = nstart, seed = seed, best = best, burnin = burnin,
#              iter = iter, thin = thin)
)
t2 <- Sys.time()
t2 - t1
ldaOutput.topics <- as.matrix(topics(ldaOutput))
write.csv(ldaOutput.topics, file = paste("output\\Lessons_2016_12_21\\lesson_topics_by_pvr.csv"))
ldaOutput.terms <- as.matrix(terms(ldaOutput, 20))
write.csv(ldaOutput.terms, file = paste("output\\Lessons_2016_12_21\\lesson_terms_by_topics.csv"))
topicProbabilities <- as.data.frame(ldaOutput@gamma)
topicProbabilities$ID <- ldaOutput@documents
write.csv(topicProbabilities, file = paste("output\\Lessons_2016_12_21\\lesson_topic_probabilities.csv"))
mergedPVR <- merge(pvr, topicProbabilities, by="ID")
write.csv(mergedPVR, file = paste("output\\Lessons_2016_12_21\\PVR_with_topics.csv"))
ldaInput <- setNames(sapply(1:length(corpus), function(i){corpus[[i]]$content}), pvr[["ID"]])
ldaInput <- sapply(ldaInput, function(x) paste(x, collapse = " "))
# tokenize on space and output as a list:
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
ldaInput <- trim(ldaInput)
document.list <- strsplit(ldaInput, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(document.list))
term.table <- sort(term.table, decreasing = TRUE)
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(document.list, get.terms)
# Compute some statistics related to the data set:
D <- length(documents) # number of documents
W <- length(vocab) # number of terms in the vocab
document.length <- sapply(documents, function(x) sum(x[2, ])) # number of tokens per document
N <- sum(document.length) # total number of tokens in the data
term.frequency <- as.integer(term.table) # frequencies of terms in the corpus
# Fit the model:
library(lda)
# MCMC and model tuning parameters:
#K <- 20
alpha <- 0.02
eta <- 0.02
set.seed(sample(1:1000000))
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = k, vocab = vocab,
#num.iterations = iter, alpha = alpha, eta = eta, initial = NULL, burnin = burnin,
alpha = alpha, eta = eta, initial = NULL,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
ldaVis <- list(phi = phi, theta = theta, doc.length = document.length, vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = ldaVis$phi,
theta = ldaVis$theta,
doc.length = ldaVis$doc.length,
vocab = ldaVis$vocab,
term.frequency = ldaVis$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = k, vocab = vocab,
#num.iterations = iter, alpha = alpha, eta = eta, initial = NULL, burnin = burnin,
alpha = alpha, eta = eta, initial = NULL,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1
burnin <- 100
iter <- 1000
alpha <- 0.02
eta <- 0.02
set.seed(sample(1:1000000))
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = k, vocab = vocab,
num.iterations = iter, alpha = alpha, eta = eta, initial = NULL, burnin = burnin,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
ldaVis <- list(phi = phi, theta = theta, doc.length = document.length, vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = ldaVis$phi,
theta = ldaVis$theta,
doc.length = ldaVis$doc.length,
vocab = ldaVis$vocab,
term.frequency = ldaVis$term.frequency)
serVis(json, out.dir = 'vis', open.browser = FALSE)
library(stm)
pvr$year <- as.numeric( format(as.Date(pvr$Report.Approval.Date., "%d-%m-%y"), "%Y"))
stmInput <- textProcessor(pvr$LESSONS, metadata = pvr, customstopwords = additionalStopWords)
stmData <- prepDocuments( stmInput$documents, stmInput$vocab, stmInput$meta, lower.thresh = 1)
t1 <- Sys.time()
stmSelectK <- searchK(stmData$documents, stmData$vocab, K = seq(5, 55, by = 1),
prevalence =~  I..CHOOSE.SECTOR + Choose.COUNTRIES + s(year) + Choose.Validation.of.Project.Program.Completion.Report.rating,
data = stmData$meta, init.type = "Spectral")
t1 <- Sys.time()
stmSelectK <- searchK(stmData$documents, stmData$vocab, K = seq(5, 105, by = 5),
prevalence =~  I..CHOOSE.SECTOR + Choose.COUNTRIES,
#+ s(year) + Choose.Validation.of.Project.Program.Completion.Report.rating,
data = stmData$meta, init.type = "Spectral")
t2 <- Sys.time()
t2 - t1
plot.searchK(stmSelectK)
t1 <- Sys.time()
stmSelectK <- searchK(stmData$documents, stmData$vocab, K = seq(2, 25, by = 1),
prevalence =~  I..CHOOSE.SECTOR + Choose.COUNTRIES,
#+ s(year) + Choose.Validation.of.Project.Program.Completion.Report.rating,
data = stmData$meta, init.type = "Spectral")
t2 <- Sys.time()
t2 - t1
plot.searchK(stmSelectK)
k <- 5
stmOutput <- stm(stmData$documents, stmData$vocab, K = k,
prevalence =~  I..CHOOSE.SECTOR + Choose.COUNTRIES + s(year) + Choose.Validation.of.Project.Program.Completion.Report.rating,
max.em.its = 500, data = stmData$meta, init.type = "Spectral")
cloud(stmOutput)
labelTopics(stmOutput)
plot.searchK(stmSelectK)
cloud(stmOutput)
cloud(stmOutput)
labelTopics(stmOutput)
plot.STM(stmOutput, type="summary")
plot.STM(stmOutput, type="hist")
plot.STM(stmOutput, type="summary", n = 25)
plot.STM(stmOutput, type="summary", n = 10)
plot.STM(stmOutput, type="perspectives", topics = 4)
plot.STM(stmOutput, type="perspectives", topics = 2)
plot.STM(stmOutput, type="perspectives", topics = c(2,4))
stmEffect <- estimateEffect(seq(1:k) ~ I..CHOOSE.SECTOR + Choose.COUNTRIES + s(year) + Choose.Validation.of.Project.Program.Completion.Report.rating,
stmOutput, metadata=stmData$meta, uncertainty = "None")
stmEffect <- estimateEffect(seq(1:k) ~ I..CHOOSE.SECTOR + Choose.COUNTRIES,
stmOutput, metadata=stmData$meta, uncertainty = "None")
stmOutput <- stm(stmData$documents, stmData$vocab, K = k,
content =~  I..CHOOSE.SECTOR + Choose.COUNTRIES + s(year) + Choose.Validation.of.Project.Program.Completion.Report.rating,
max.em.its = 500, data = stmData$meta, init.type = "Spectral")
stmEffect <- estimateEffect(seq(1:k) ~ I..CHOOSE.SECTOR + Choose.COUNTRIES,
stmOutput, metadata=stmData$meta, uncertainty = "None")
stmOutput <- stm(stmData$documents, stmData$vocab, K = k,
prevalence =~  I..CHOOSE.SECTOR + Choose.COUNTRIES,
max.em.its = 500, data = stmData$meta, init.type = "Spectral")
cloud(stmOutput)
labelTopics(stmOutput)
plot.STM(stmOutput, type="summary", n = 10)
plot.STM(stmOutput, type="perspectives", topics = c(2,4))
stmEffect <- estimateEffect(seq(1:k) ~ I..CHOOSE.SECTOR + Choose.COUNTRIES,
stmOutput, metadata=stmData$meta, uncertainty = "None")
stmEffect <- estimateEffect(seq(1:k) ~ I..CHOOSE.SECTOR,
stmOutput, metadata=stmData$meta, uncertainty = "None")
stmTopicCorr <- topicCorr(stmOutput)
plot.topicCorr(stmTopicCorr)
R.version
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv", header = TRUE, quote="\"", stringsAsFactors= TRUE, strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
library(tm)
corpus <- Corpus(VectorSource(lessons))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[[:cntrl:]]", replacement = " ")
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "[^[:alnum:]]", replacement = " ")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(gsub),
pattern = "[[:space:]]{1,}(xc|xl|l?x{0,3})(ix|iv|v?i{0,3})[[:space:]]{1,}",
replacement = " ") #remove roman numerals
corpus <- tm_map(corpus, removeWords, stopwords("SMART"))
additionalStopWords <- c("adb", "amp", "completion report", "document", "lesson", "lessons", "para", "pcr",
"program", "project", "project completion report", "projects", "report", "text",
"topic", "topics", "xarr", "year")
corpus <- tm_map(corpus, removeWords, additionalStopWords)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(gsub), pattern = "^\\s+|\\s+$", replacement = "")
#Temporarily removing stemming for ease of interpretation
corpus <- tm_map(corpus, stemDocument)
write(sapply(1:length(corpus), function(i){corpus[[i]]$content}), "pvr_lessons_stem.txt")
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv", header = TRUE, quote="\"", stringsAsFactors= TRUE, strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
pattern <- "project cycle stage:*\n"
regexpr(pattern, lessons)
pattern <- "project cycle stage:"
regexpr(pattern, lessons)
m <- regexpr(pattern, lessons)
regmatches(lessons, m)
pattern <- "project cycle stage:.*"
m <- regexpr(pattern, lessons)
regmatches(lessons, m)
pattern <- "project cycle stage:.*\r?\n"
m <- regexpr(pattern, lessons)
regmatches(lessons, m)
library(tm)
stopwords("SMART")
pvr <- read.csv("input\\ADB_Comprehensive_PVR.csv", header = TRUE, quote="\"", stringsAsFactors= TRUE, strip.white = TRUE)
#Extract contents of the Project Title column
lessons <- pvr[["LESSONS"]]
lessons
stopwords_regex = paste(stopwords("SMART"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
lessons = stringr::str_replace_all(lessons, stopwords_regex, '')
lessons
lessons <- pvr[["LESSONS"]]
lessons <- pvr[["LESSONS"]]
lessons{{1}}
lessons[[1]]
stopwords_regex = paste(stopwords("SMART"), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
lessons[[1]] = stringr::str_replace_all(lessons[[1]], stopwords_regex, '')
lessons = stringr::str_replace_all(lessons, stopwords_regex, '')
lessons[[1]]
lessons <- pvr[["LESSONS"]]
